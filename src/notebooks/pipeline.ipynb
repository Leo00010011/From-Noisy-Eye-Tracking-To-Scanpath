{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b863169",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ee9b4",
   "metadata": {},
   "source": [
    "# Imports\n",
    "## Pip Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4aebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    new_directory_path = \"..\\\\..\\\\\"\n",
    "    os.chdir(new_directory_path)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57717557",
   "metadata": {},
   "source": [
    "## My Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea0616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import CocoFreeView\n",
    "from src.simulation import gen_gaze, downsample\n",
    "from src.noise import add_random_center_correlated_radial_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7b40b",
   "metadata": {},
   "source": [
    "# Code\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795ed3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(mask, fixation, side = 'right'):\n",
    "    start = 0\n",
    "    stop = mask.shape[0]\n",
    "    step = 1\n",
    "    if side == 'left':\n",
    "        start = mask.shape[0] - 1 \n",
    "        stop = -1\n",
    "        step = -1\n",
    "\n",
    "    for i in range(start,stop, step):\n",
    "        if mask[i] == fixation:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def test_segment_is_inside(x, si,ei,gaze, fixation_mask):\n",
    "    sidx = search(fixation_mask, si + 1, side = 'right')\n",
    "    eidx = search(fixation_mask, ei + 1, side = 'left')\n",
    "    if sidx == -1:\n",
    "        print(f'❌ Start Fixation not found: si:{si + 1} \\n {fixation_mask}')\n",
    "    if eidx == -1:\n",
    "        print(f'❌ End Fixation not found: si:{si + 1} \\n {fixation_mask}')\n",
    "    # print(fixation_mask.shape)\n",
    "    # print(si)\n",
    "    # print(ei)\n",
    "    # print(sidx)\n",
    "    # print(eidx)\n",
    "    if x[2,0] <= gaze[2,sidx] and (x[2,-1] + 200) >= gaze[2,eidx]:\n",
    "        print(f'✅Pass: DS [{x[2,0]},{x[2,-1]}] Ori [{gaze[2,sidx]},{gaze[2,eidx]}]')\n",
    "    else:\n",
    "        print(f'❌Outside: DS [{x[2,0]},{x[2,-1]}] Ori [{gaze[2,sidx]},{gaze[2,eidx]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd117c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4012500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compute image embeddings just once\n",
    "# TODO Refactor the dataset class (TOO LARGE)\n",
    "# TODO Review the outputs in the validation and with some tests\n",
    "class PathCocoFreeViewDatasetBatch(Dataset):\n",
    "    '''\n",
    "    The noisy and downsampled simulated eye-tracking and the section of the scanpath that fits entirely in that part\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_path = 'data\\\\Coco FreeView',\n",
    "                 sample_size=-1,\n",
    "                 sampling_rate=60,\n",
    "                 downsample=200,\n",
    "                 batch_size=128,\n",
    "                 min_scanpath_duration = 3000,\n",
    "                 max_fixation_duration = 1200,\n",
    "                 log = False,\n",
    "                 debug = False):\n",
    "        super().__init__()\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.sample_size = sample_size # 90% larger than 20 at downsample 200\n",
    "        self.downsample = downsample\n",
    "        self.min_scanpath_duration = min_scanpath_duration\n",
    "        self.max_fixation_duration = max_fixation_duration\n",
    "        self.log = log\n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "        self.ori_path = os.path.join(data_path, 'dataset.hdf5')\n",
    "        self.ori_data = None        \n",
    "        if not os.path.exists(self.ori_path):\n",
    "            if self.log:\n",
    "                print('gen data file not found')\n",
    "            self.__preprocess()\n",
    "        self.shuffled_path = self.ori_path.replace('.hdf5', '_shuffled.hdf5')\n",
    "        self.shuffled_data = None\n",
    "        self.json_dataset = None\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.ori_path,'r') as ori_data:\n",
    "            return ori_data['down_gaze'].shape[0]//self.batch_size\n",
    "    \n",
    "    def sample_count(self):\n",
    "        with h5py.File(self.ori_path,'r') as ori_data:\n",
    "            return ori_data['down_gaze'].shape[0]\n",
    "\n",
    "\n",
    "    def __preprocess(self):\n",
    "        # TODO gen all the clean samples\n",
    "        data = self.json_dataset\n",
    "        gen_data = []\n",
    "        original_data_count = len(data)\n",
    "        for index in range(original_data_count):\n",
    "            gaze, fixations, fixation_mask = gen_gaze(data,\n",
    "                                                        index, self.sampling_rate,\n",
    "                                                        get_scanpath=True,\n",
    "                                                        get_fixation_mask=True)\n",
    "            down_gaze = downsample(gaze, down_time_step=self.downsample)\n",
    "            if (gaze[2,-1] < max(self.min_scanpath_duration, (self.sample_size - 1)*self.downsample) or\n",
    "                fixations[2].max() > self.max_fixation_duration) :\n",
    "                continue\n",
    "            gen_data.append({'down_gaze': down_gaze,\n",
    "                             'fixations': fixations,\n",
    "                             'fixation_mask': fixation_mask,\n",
    "                             'gaze': gaze})\n",
    "        if self.log:\n",
    "            removed = original_data_count - len(gen_data)\n",
    "            print(f'Removed: {removed} - {(removed/original_data_count)*100}% ')\n",
    "        # save\n",
    "        with h5py.File(self.ori_path, 'w') as f:\n",
    "            # Create datasets with shape (43000,) and the vlen dtype\n",
    "            item = gen_data[0]\n",
    "            k_dset = dict()\n",
    "            for k in item.keys():\n",
    "                k_dset[k] = f.create_dataset(k, len(gen_data), dtype= h5py.special_dtype(vlen= item[k].dtype))\n",
    "\n",
    "\n",
    "            # Loop and store each item\n",
    "            for i, item in enumerate(gen_data):\n",
    "                for k in item.keys():\n",
    "                    if item[k].ndim > 1:\n",
    "                        k_dset[k][i] = item[k].flatten()\n",
    "                    else:\n",
    "                        k_dset[k][i] = item[k]\n",
    "        \n",
    "        if self.log:\n",
    "            print('generated items saved')\n",
    "\n",
    "\n",
    "    def __extract_random_period(self, size, noisy_samples, fixations, fixation_mask):\n",
    "        down_idx = np.random.randint(0, noisy_samples.shape[1] - size + 1, 1, dtype = int)[0]\n",
    "        # get the values in the original sampling rate\n",
    "        conversion_factor = self.downsample/(1000/self.sampling_rate)\n",
    "        ori_idx = math.floor(down_idx*conversion_factor)\n",
    "        ori_size = math.ceil((size - 1)*conversion_factor)\n",
    "        last_idx = ori_idx + ori_size\n",
    "        # TEST\n",
    "        # get the fisrt fixation and if it is not completely included get the next one\n",
    "        if fixation_mask[ori_idx] > 0:\n",
    "            if (ori_idx - 1) >= 0 and fixation_mask[ori_idx - 1] == fixation_mask[ori_idx]:\n",
    "                start_fixation = fixation_mask[ori_idx] + 1\n",
    "            else:\n",
    "                start_fixation = fixation_mask[ori_idx]\n",
    "        else:\n",
    "            # if the first value is a saccade look for the first fixation\n",
    "            current_idx = ori_idx + 1\n",
    "            while current_idx < (ori_idx + ori_size) and fixation_mask[current_idx] == 0:\n",
    "                current_idx += 1\n",
    "            if current_idx == (ori_idx + ori_size):\n",
    "                # if there is not a fixation return an empty array\n",
    "                return noisy_samples[:, down_idx:down_idx + size],np.array([]), -1,-1\n",
    "            else:\n",
    "                # TEST\n",
    "                start_fixation = fixation_mask[current_idx]\n",
    "        # search the last fixation\n",
    "        if fixation_mask[last_idx] > 0:\n",
    "            if (last_idx + 1) < fixation_mask.shape[0] and fixation_mask[last_idx + 1] == fixation_mask[last_idx]:\n",
    "                end_fixation = fixation_mask[last_idx] - 1\n",
    "            else:\n",
    "                end_fixation = fixation_mask[last_idx]\n",
    "        else:\n",
    "            current_idx = last_idx - 1\n",
    "            while current_idx > ori_idx and fixation_mask[current_idx] == 0:\n",
    "                current_idx -= 1\n",
    "            end_fixation = fixation_mask[current_idx]\n",
    "        # the mask are saved shifted in order to assign 0 to the saccade samples\n",
    "        start_fixation -= 1\n",
    "        end_fixation -= 1\n",
    "        x = noisy_samples[:, down_idx:down_idx + size]\n",
    "        y = fixations[:, start_fixation: end_fixation + 1]\n",
    "        return x, y, start_fixation, end_fixation\n",
    "\n",
    "    def gen_single_item(self, index):\n",
    "        if self.json_dataset is None:\n",
    "            self.json_dataset = CocoFreeView()\n",
    "        data = self.json_dataset\n",
    "        gaze, fixations, fixation_mask = gen_gaze(data,\n",
    "                                                        index, self.sampling_rate,\n",
    "                                                        get_scanpath=True,\n",
    "                                                        get_fixation_mask=True)\n",
    "        x = downsample(gaze, down_time_step=self.downsample)\n",
    "        y = fixations\n",
    "        if self.sample_size != -1:\n",
    "            x, y, start_fixation, end_fixation = self.__extract_random_period(self.sample_size,\n",
    "                                                x,\n",
    "                                                y,\n",
    "                                                fixation_mask)\n",
    "            \n",
    "        x, _ = add_random_center_correlated_radial_noise(x, [320//2, 512//2], 1/16,\n",
    "                                                                  radial_corr=.2,\n",
    "                                                                  radial_avg_norm=4.13,\n",
    "                                                                  radial_std=3.5,\n",
    "                                                                  center_noise_std=100,\n",
    "                                                                  center_corr=.3,\n",
    "                                                                  center_delta_norm=300,\n",
    "                                                                  center_delta_r=.3)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def get_single_item(self, index):\n",
    "        # reading is inefficient because is reading from memory one by one\n",
    "        if self.ori_data is None:\n",
    "            self.ori_data = h5py.File(self.ori_path,'r')\n",
    "        down_gaze = self.ori_data['down_gaze'][index].reshape((3,-1))\n",
    "        fixations = self.ori_data['fixations'][index].reshape((3,-1))\n",
    "        x = down_gaze\n",
    "        y = fixations\n",
    "        if self.sample_size != -1:\n",
    "            fixation_mask = self.ori_data['fixation_mask'][index]\n",
    "            x, y, start_fixation, end_fixation = self.__extract_random_period(self.sample_size,\n",
    "                                                x,\n",
    "                                                fixations,\n",
    "                                                fixation_mask)\n",
    "            # if start_fixation != -1:\n",
    "            #     gaze = self.ori_data['gaze'][index].reshape((3,-1))\n",
    "            #     test_segment_is_inside(x,start_fixation, end_fixation,gaze, fixation_mask)\n",
    "\n",
    "        x, _ = add_random_center_correlated_radial_noise(x, [320//2, 512//2], 1/16,\n",
    "                                                                  radial_corr=.2,\n",
    "                                                                  radial_avg_norm=4.13,\n",
    "                                                                  radial_std=3.5,\n",
    "                                                                  center_noise_std=100,\n",
    "                                                                  center_corr=.3,\n",
    "                                                                  center_delta_norm=300,\n",
    "                                                                  center_delta_r=.3)\n",
    "        return x, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # reading is 3x faster with batch size 128 \n",
    "        # but can´t use the workers of the torch.dataloader (epoch in 4.9)\n",
    "        if self.shuffled_data is None:\n",
    "            self.shuffled_data = h5py.File(self.shuffled_path,'r')\n",
    "        batch_size = self.batch_size\n",
    "        down_gaze = self.shuffled_data['down_gaze'][index*batch_size:(index + 1)*batch_size]\n",
    "        fixations = self.shuffled_data['fixations'][index*batch_size:(index + 1)*batch_size]\n",
    "        vals = None\n",
    "        if self.sample_size != -1:\n",
    "            fixation_mask = self.shuffled_data['fixation_mask'][index*batch_size:(index + 1)*batch_size]\n",
    "            # gaze = self.data['gaze'][index]\n",
    "            # vals = (down_gaze,fixations,fixation_mask, gaze)\n",
    "            vals = (down_gaze,fixations,fixation_mask)\n",
    "        else:\n",
    "            vals = (down_gaze,fixations)\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for value in zip(*vals):\n",
    "            x = value[0].reshape((3,-1))        \n",
    "            y = value[1].reshape((3,-1))\n",
    "            if self.sample_size != -1:\n",
    "                fixation_mask = value[2]\n",
    "                x, y, start_fixation, end_fixation = self.__extract_random_period(self.sample_size,\n",
    "                                                    x,\n",
    "                                                    y,\n",
    "                                                    fixation_mask)\n",
    "                # if start_fixation != -1:\n",
    "                #     gaze = value[3].reshape((3,-1))\n",
    "                #     test_segment_is_inside(x,start_fixation, end_fixation,gaze, fixation_mask)\n",
    "\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "        x_batch, _ = add_random_center_correlated_radial_noise(x_batch, [320//2, 512//2], 1/16,\n",
    "                                                                radial_corr=.2,\n",
    "                                                                radial_avg_norm=4.13,\n",
    "                                                                radial_std=3.5,\n",
    "                                                                center_noise_std=100,\n",
    "                                                                center_corr=.3,\n",
    "                                                                center_delta_norm=300,\n",
    "                                                                center_delta_r=.3)\n",
    "        # self.close_and_remove_data()\n",
    "        self.shuffled_data.close()\n",
    "        self.shuffled_data = None\n",
    "        \n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    def shuffle_dataset(self):\n",
    "        \n",
    "        with h5py.File(self.ori_path,'r') as ori_data:\n",
    "            dataset_names = ['down_gaze', 'fixations', 'fixation_mask', 'gaze']\n",
    "            if self.log:\n",
    "                print('reading original data')\n",
    "            original_data = {name: ori_data[name][:] for name in dataset_names}\n",
    "        idx = np.arange(original_data['down_gaze'].shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        for name in dataset_names:\n",
    "            original_data[name] = original_data[name][idx]\n",
    "\n",
    "        with h5py.File(self.shuffled_path, 'w') as f_out:\n",
    "            for name, data in original_data.items():\n",
    "                f_out.create_dataset(\n",
    "                    name,\n",
    "                    data=data,\n",
    "                )\n",
    "        if self.log:\n",
    "            print('shuffled data saved')\n",
    "\n",
    "    def close_and_remove_data(self):\n",
    "        if self.ori_data is not None:\n",
    "            # if self.log:\n",
    "            #     print('closing original data file')\n",
    "            self.ori_data.close()\n",
    "            self.ori_data = None\n",
    "        if self.shuffled_data is not None:\n",
    "            # if self.log:\n",
    "            #     print('closing shuffled data file')\n",
    "            self.shuffled_data.close()\n",
    "            self.shuffled_data = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e60884",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0f8df",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f33bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PathCocoFreeViewDatasetBatch(sample_size= 8,log = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf6ba0",
   "metadata": {},
   "source": [
    "## Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83410f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39869/39869 [00:19<00:00, 2000.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(dataset.sample_count())):\n",
    "    dataset.get_single_item(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39869 [00:00<?, ?it/s]C:\\Users\\ulloa\\AppData\\Local\\Temp\\ipykernel_15260\\2170223089.py:127: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  end_fixation -= 1\n",
      "C:\\Users\\ulloa\\AppData\\Local\\Temp\\ipykernel_15260\\2170223089.py:129: RuntimeWarning: overflow encountered in scalar add\n",
      "  y = fixations[:, start_fixation: end_fixation + 1]\n",
      "  3%|▎         | 1344/39869 [00:00<00:13, 2758.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 893: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2183/39869 [00:00<00:13, 2772.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 1682: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2736/39869 [00:01<00:13, 2738.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 2493: high <= 0\n",
      "Error processing item 2877: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3550/39869 [00:01<00:13, 2661.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 3219: high <= 0\n",
      "Error processing item 3516: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 4644/39869 [00:01<00:13, 2636.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 4288: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 5454/39869 [00:02<00:12, 2655.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 4966: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 6824/39869 [00:02<00:12, 2722.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 6425: high <= 0\n",
      "Error processing item 6488: high <= 0\n",
      "Error processing item 6975: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 7931/39869 [00:02<00:11, 2741.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 7404: high <= 0\n",
      "Error processing item 7511: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 8478/39869 [00:03<00:11, 2702.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 8009: high <= 0\n",
      "Error processing item 8283: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 9298/39869 [00:03<00:11, 2706.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 8995: high <= 0\n",
      "Error processing item 9118: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 10387/39869 [00:03<00:11, 2617.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 10065: high <= 0\n",
      "Error processing item 10206: high <= 0\n",
      "Error processing item 10373: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11186/39869 [00:04<00:11, 2600.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 10762: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 13118/39869 [00:04<00:09, 2755.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 12769: high <= 0\n",
      "Error processing item 13011: high <= 0\n",
      "Error processing item 13111: high <= 0\n",
      "Error processing item 13126: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 14251/39869 [00:05<00:09, 2792.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 13721: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 14806/39869 [00:05<00:09, 2707.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 14562: high <= 0\n",
      "Error processing item 14819: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 15372/39869 [00:05<00:09, 2539.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 15138: high <= 0\n",
      "Error processing item 15412: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 16463/39869 [00:06<00:08, 2671.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 15979: high <= 0\n",
      "Error processing item 16031: high <= 0\n",
      "Error processing item 16405: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 17000/39869 [00:06<00:08, 2663.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 16587: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 17803/39869 [00:06<00:08, 2642.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 17393: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 18622/39869 [00:07<00:07, 2699.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 18219: high <= 0\n",
      "Error processing item 18457: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 19423/39869 [00:07<00:07, 2566.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 18995: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 20215/39869 [00:07<00:07, 2591.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 19784: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 20740/39869 [00:07<00:07, 2416.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 20415: high <= 0\n",
      "Error processing item 20448: high <= 0\n",
      "Error processing item 20508: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 21790/39869 [00:08<00:07, 2578.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 21324: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 22309/39869 [00:08<00:06, 2552.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 21924: high <= 0\n",
      "Error processing item 21997: high <= 0\n",
      "Error processing item 22030: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 23397/39869 [00:08<00:06, 2677.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 23068: high <= 0\n",
      "Error processing item 23414: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 24499/39869 [00:09<00:05, 2675.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 24149: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 25033/39869 [00:09<00:05, 2608.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 24722: high <= 0\n",
      "Error processing item 24944: high <= 0\n",
      "Error processing item 25253: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 25849/39869 [00:09<00:05, 2666.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 25289: high <= 0\n",
      "Error processing item 25304: high <= 0\n",
      "Error processing item 25389: high <= 0\n",
      "Error processing item 25426: high <= 0\n",
      "Error processing item 25592: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 26116/39869 [00:09<00:05, 2643.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 25941: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 29152/39869 [00:11<00:03, 2712.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 28740: high <= 0\n",
      "Error processing item 28833: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31103/39869 [00:11<00:03, 2772.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 30526: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 33354/39869 [00:12<00:02, 2802.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 32918: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 34489/39869 [00:12<00:01, 2804.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 33965: high <= 0\n",
      "Error processing item 34001: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35045/39869 [00:13<00:01, 2736.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 34543: high <= 0\n",
      "Error processing item 35014: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 35595/39869 [00:13<00:01, 2529.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 35319: high <= 0\n",
      "Error processing item 35356: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 36664/39869 [00:13<00:01, 2528.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 36227: high <= 0\n",
      "Error processing item 36473: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 37200/39869 [00:14<00:01, 2593.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 36891: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 38581/39869 [00:14<00:00, 2733.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 38182: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 39420/39869 [00:14<00:00, 2766.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing item 38934: high <= 0\n",
      "Error processing item 39121: high <= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39869/39869 [00:15<00:00, 2652.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(dataset.sample_count())):\n",
    "    try:\n",
    "        dataset.gen_single_item(i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea8ec4",
   "metadata": {},
   "source": [
    "## Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34412f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading original data\n",
      "shuffled data saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [00:03<00:00, 103.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO The multiprocessing dataloader (sharing the hdf5 files through multitable)\n",
    "\n",
    "dataset.close_and_remove_data()\n",
    "dataset.shuffle_dataset()\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b84dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closing data files\n",
      "closing data files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/311 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2144, 11824, 18660) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1284\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m dataset\u001b[38;5;241m.\u001b[39mclose_and_remove_data()\n\u001b[0;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1491\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1453\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1449\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1450\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1453\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1455\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ulloa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1297\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1296\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2144, 11824, 18660) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset.close_and_remove_data()\n",
    "dataset.shuffled_data\n",
    "dataset.close_and_remove_data()\n",
    "dataloader = DataLoader(dataset, batch_size=None, shuffle=False, num_workers=3)\n",
    "for batch in tqdm(dataloader):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
