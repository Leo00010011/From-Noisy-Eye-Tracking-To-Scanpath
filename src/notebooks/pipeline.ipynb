{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b863169",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ee9b4",
   "metadata": {},
   "source": [
    "# Imports\n",
    "## Pip Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd4aebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    new_directory_path = \"..\\\\..\\\\\"\n",
    "    os.chdir(new_directory_path)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57717557",
   "metadata": {},
   "source": [
    "## My Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea0616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets import CocoFreeView\n",
    "from src.preprocess.simulation import gen_gaze, downsample\n",
    "from src.preprocess.noise import add_random_center_correlated_radial_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c7b40b",
   "metadata": {},
   "source": [
    "# Code\n",
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795ed3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(mask, fixation, side = 'right'):\n",
    "    start = 0\n",
    "    stop = mask.shape[0]\n",
    "    step = 1\n",
    "    if side == 'left':\n",
    "        start = mask.shape[0] - 1 \n",
    "        stop = -1\n",
    "        step = -1\n",
    "\n",
    "    for i in range(start,stop, step):\n",
    "        if mask[i] == fixation:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def test_segment_is_inside(x, si,ei,gaze, fixation_mask):\n",
    "    sidx = search(fixation_mask, si + 1, side = 'right')\n",
    "    eidx = search(fixation_mask, ei + 1, side = 'left')\n",
    "    if sidx == -1:\n",
    "        print(f'❌ Start Fixation not found: si:{si + 1} \\n {fixation_mask}')\n",
    "    if eidx == -1:\n",
    "        print(f'❌ End Fixation not found: si:{si + 1} \\n {fixation_mask}')\n",
    "    # print(fixation_mask.shape)\n",
    "    # print(si)\n",
    "    # print(ei)\n",
    "    # print(sidx)\n",
    "    # print(eidx)\n",
    "    if x[2,0] <= gaze[2,sidx] and (x[2,-1] + 200) >= gaze[2,eidx]:\n",
    "        print(f'✅Pass: DS [{x[2,0]},{x[2,-1]}] Ori [{gaze[2,sidx]},{gaze[2,eidx]}]')\n",
    "    else:\n",
    "        print(f'❌Outside: DS [{x[2,0]},{x[2,-1]}] Ori [{gaze[2,sidx]},{gaze[2,eidx]}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd117c",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4012500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Compute image embeddings just once\n",
    "# TODO Refactor the dataset class (TOO LARGE)\n",
    "# TODO Review the outputs in the validation and with some tests\n",
    "class PathCocoFreeViewDatasetBatch(Dataset):\n",
    "    '''\n",
    "    The noisy and downsampled simulated eye-tracking and the section of the scanpath that fits entirely in that part\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_path = 'data\\\\Coco FreeView',\n",
    "                 sample_size=-1,\n",
    "                 sampling_rate=60,\n",
    "                 downsample_int=200,\n",
    "                 batch_size=128,\n",
    "                 min_scanpath_duration = 3000,\n",
    "                 max_fixation_duration = 1200,\n",
    "                 log = False,\n",
    "                 debug = False):\n",
    "        super().__init__()\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.sample_size = sample_size # 90% larger than 20 at downsample 200\n",
    "        self.downsample = downsample_int\n",
    "        self.min_scanpath_duration = min_scanpath_duration\n",
    "        self.max_fixation_duration = max_fixation_duration\n",
    "        self.log = log\n",
    "        self.batch_size = batch_size\n",
    "        self.debug = debug\n",
    "        self.ori_path = os.path.join(data_path, 'dataset.hdf5')\n",
    "        self.ori_data = None        \n",
    "        if not os.path.exists(self.ori_path):\n",
    "            print('Execute preprocess')\n",
    "        self.shuffled_path = self.ori_path.replace('.hdf5', '_shuffled.hdf5')\n",
    "        self.shuffled_data = None\n",
    "        self.json_dataset = None\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        with h5py.File(self.ori_path,'r') as ori_data:\n",
    "            return math.ceil(ori_data['down_gaze'].shape[0]/self.batch_size)\n",
    "    \n",
    "    def sample_count(self):\n",
    "        with h5py.File(self.ori_path,'r') as ori_data:\n",
    "            return ori_data['down_gaze'].shape[0]\n",
    "\n",
    "\n",
    "    def __extract_random_period(self, size, noisy_samples, fixations, fixation_mask):\n",
    "        down_idx = np.random.randint(0, noisy_samples.shape[1] - size + 1, 1, dtype = int)[0]\n",
    "        # get the values in the original sampling rate\n",
    "        conversion_factor = self.downsample/(1000/self.sampling_rate)\n",
    "        ori_idx = math.floor(down_idx*conversion_factor)\n",
    "        ori_size = math.ceil((size - 1)*conversion_factor)\n",
    "        last_idx = ori_idx + ori_size\n",
    "        # TEST\n",
    "        # get the fisrt fixation and if it is not completely included get the next one\n",
    "        if fixation_mask[ori_idx] > 0:\n",
    "            if (ori_idx - 1) >= 0 and fixation_mask[ori_idx - 1] == fixation_mask[ori_idx]:\n",
    "                start_fixation = fixation_mask[ori_idx] + 1\n",
    "            else:\n",
    "                start_fixation = fixation_mask[ori_idx]\n",
    "        else:\n",
    "            # if the first value is a saccade look for the first fixation\n",
    "            current_idx = ori_idx + 1\n",
    "            while current_idx < (ori_idx + ori_size) and fixation_mask[current_idx] == 0:\n",
    "                current_idx += 1\n",
    "            if current_idx == (ori_idx + ori_size):\n",
    "                # if there is not a fixation return an empty array\n",
    "                return noisy_samples[:, down_idx:down_idx + size],np.array([]), -1,-1\n",
    "            else:\n",
    "                # TEST\n",
    "                start_fixation = fixation_mask[current_idx]\n",
    "        # search the last fixation\n",
    "        if fixation_mask[last_idx] > 0:\n",
    "            if (last_idx + 1) < fixation_mask.shape[0] and fixation_mask[last_idx + 1] == fixation_mask[last_idx]:\n",
    "                end_fixation = fixation_mask[last_idx] - 1\n",
    "            else:\n",
    "                end_fixation = fixation_mask[last_idx]\n",
    "        else:\n",
    "            current_idx = last_idx - 1\n",
    "            while current_idx > ori_idx and fixation_mask[current_idx] == 0:\n",
    "                current_idx -= 1\n",
    "            end_fixation = fixation_mask[current_idx]\n",
    "        # the mask are saved shifted in order to assign 0 to the saccade samples\n",
    "        start_fixation -= 1\n",
    "        end_fixation -= 1\n",
    "        x = noisy_samples[:, down_idx:down_idx + size]\n",
    "        y = fixations[:, start_fixation: end_fixation + 1]\n",
    "        return x, y, start_fixation, end_fixation\n",
    "\n",
    "\n",
    "    def get_single_item(self, index):\n",
    "        # reading is inefficient because is reading from memory one by one\n",
    "        if self.ori_data is None:\n",
    "            self.ori_data = h5py.File(self.ori_path,'r')\n",
    "        down_gaze = self.ori_data['down_gaze'][index].reshape((3,-1))\n",
    "        fixations = self.ori_data['fixations'][index].reshape((3,-1))\n",
    "        x = down_gaze\n",
    "        y = fixations\n",
    "        if self.sample_size != -1:\n",
    "            fixation_mask = self.ori_data['fixation_mask'][index]\n",
    "            x, y, start_fixation, end_fixation = self.__extract_random_period(self.sample_size,\n",
    "                                                x,\n",
    "                                                fixations,\n",
    "                                                fixation_mask)\n",
    "            # if start_fixation != -1:\n",
    "            #     gaze = self.ori_data['gaze'][index].reshape((3,-1))\n",
    "            #     test_segment_is_inside(x,start_fixation, end_fixation,gaze, fixation_mask)\n",
    "\n",
    "        x, _ = add_random_center_correlated_radial_noise(x, [320//2, 512//2], 1/16,\n",
    "                                                                  radial_corr=.2,\n",
    "                                                                  radial_avg_norm=4.13,\n",
    "                                                                  radial_std=3.5,\n",
    "                                                                  center_noise_std=100,\n",
    "                                                                  center_corr=.3,\n",
    "                                                                  center_delta_norm=300,\n",
    "                                                                  center_delta_r=.3)\n",
    "        return x, y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # reading is 3x faster with batch size 128 \n",
    "        # but can´t use the workers of the torch.dataloader (epoch in 4.9)\n",
    "        if self.shuffled_data is None:\n",
    "            self.shuffled_data = h5py.File(self.shuffled_path,'r')\n",
    "        batch_size = self.batch_size\n",
    "        down_gaze = self.shuffled_data['down_gaze'][index*batch_size:(index + 1)*batch_size]\n",
    "        fixations = self.shuffled_data['fixations'][index*batch_size:(index + 1)*batch_size]\n",
    "        vals = None\n",
    "        if self.sample_size != -1:\n",
    "            fixation_mask = self.shuffled_data['fixation_mask'][index*batch_size:(index + 1)*batch_size]\n",
    "            # gaze = self.data['gaze'][index]\n",
    "            # vals = (down_gaze,fixations,fixation_mask, gaze)\n",
    "            vals = (down_gaze,fixations,fixation_mask)\n",
    "        else:\n",
    "            vals = (down_gaze,fixations)\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for value in zip(*vals):\n",
    "            x = value[0].reshape((3,-1))        \n",
    "            y = value[1].reshape((3,-1))\n",
    "            if self.sample_size != -1:\n",
    "                fixation_mask = value[2]\n",
    "                x, y, start_fixation, end_fixation = self.__extract_random_period(self.sample_size,\n",
    "                                                    x,\n",
    "                                                    y,\n",
    "                                                    fixation_mask)\n",
    "                # if start_fixation != -1:\n",
    "                #     gaze = value[3].reshape((3,-1))\n",
    "                #     test_segment_is_inside(x,start_fixation, end_fixation,gaze, fixation_mask)\n",
    "\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "        x_batch, _ = add_random_center_correlated_radial_noise(x_batch, [320//2, 512//2], 1/16,\n",
    "                                                                radial_corr=.2,\n",
    "                                                                radial_avg_norm=4.13,\n",
    "                                                                radial_std=3.5,\n",
    "                                                                center_noise_std=100,\n",
    "                                                                center_corr=.3,\n",
    "                                                                center_delta_norm=300,\n",
    "                                                                center_delta_r=.3)\n",
    "        # self.close_and_remove_data()\n",
    "        self.shuffled_data.close()\n",
    "        self.shuffled_data = None\n",
    "        \n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    def shuffle_dataset(self):\n",
    "        \n",
    "        with h5py.File(self.ori_path,'r') as ori_data:\n",
    "            dataset_names = ['down_gaze', 'fixations', 'fixation_mask', 'gaze']\n",
    "            if self.log:\n",
    "                print('reading original data')\n",
    "            original_data = {name: ori_data[name][:] for name in dataset_names}\n",
    "        idx = np.arange(original_data['down_gaze'].shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        for name in dataset_names:\n",
    "            original_data[name] = original_data[name][idx]\n",
    "\n",
    "        with h5py.File(self.shuffled_path, 'w') as f_out:\n",
    "            for name, data in original_data.items():\n",
    "                f_out.create_dataset(\n",
    "                    name,\n",
    "                    data=data,\n",
    "                )\n",
    "        if self.log:\n",
    "            print('shuffled data saved')\n",
    "\n",
    "    def close_and_remove_data(self):\n",
    "        if self.ori_data is not None:\n",
    "            # if self.log:\n",
    "            #     print('closing original data file')\n",
    "            self.ori_data.close()\n",
    "            self.ori_data = None\n",
    "        if self.shuffled_data is not None:\n",
    "            # if self.log:\n",
    "            #     print('closing shuffled data file')\n",
    "            self.shuffled_data.close()\n",
    "            self.shuffled_data = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cd2a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryDataset(Dataset):\n",
    "    def __init__(self, data_path = 'data\\\\Coco FreeView', sample_size=-1, log = False, sampling_rate=60, downsample_int=200):\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.downsample = downsample_int\n",
    "        self.sample_size = sample_size\n",
    "        file_path = os.path.join(data_path, 'dataset.hdf5')\n",
    "        self.data_path = data_path\n",
    "        self.log = log\n",
    "        self.data_store = {}\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            for key in f.keys():\n",
    "                self.data_store[key] = f[key][:] # [:] reads all data\n",
    "        if self.log:\n",
    "            print('Data loaded in memory')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_store['down_gaze'].shape[0]\n",
    "        \n",
    "\n",
    "    def __extract_random_period(self, size, noisy_samples, fixations, fixation_mask):\n",
    "        down_idx = np.random.randint(0, noisy_samples.shape[1] - size + 1, 1, dtype = int)[0]\n",
    "        # get the values in the original sampling rate\n",
    "        conversion_factor = self.downsample/(1000/self.sampling_rate)\n",
    "        ori_idx = math.floor(down_idx*conversion_factor)\n",
    "        ori_size = math.ceil((size - 1)*conversion_factor)\n",
    "        last_idx = ori_idx + ori_size\n",
    "        # TEST\n",
    "        # get the fisrt fixation and if it is not completely included get the next one\n",
    "        if fixation_mask[ori_idx] > 0:\n",
    "            if (ori_idx - 1) >= 0 and fixation_mask[ori_idx - 1] == fixation_mask[ori_idx]:\n",
    "                start_fixation = fixation_mask[ori_idx] + 1\n",
    "            else:\n",
    "                start_fixation = fixation_mask[ori_idx]\n",
    "        else:\n",
    "            # if the first value is a saccade look for the first fixation\n",
    "            current_idx = ori_idx + 1\n",
    "            while current_idx < (ori_idx + ori_size) and fixation_mask[current_idx] == 0:\n",
    "                current_idx += 1\n",
    "            if current_idx == (ori_idx + ori_size):\n",
    "                # if there is not a fixation return an empty array\n",
    "                return noisy_samples[:, down_idx:down_idx + size],np.array([]), -1,-1\n",
    "            else:\n",
    "                # TEST\n",
    "                start_fixation = fixation_mask[current_idx]\n",
    "        # search the last fixation\n",
    "        if fixation_mask[last_idx] > 0:\n",
    "            if (last_idx + 1) < fixation_mask.shape[0] and fixation_mask[last_idx + 1] == fixation_mask[last_idx]:\n",
    "                end_fixation = fixation_mask[last_idx] - 1\n",
    "            else:\n",
    "                end_fixation = fixation_mask[last_idx]\n",
    "        else:\n",
    "            current_idx = last_idx - 1\n",
    "            while current_idx > ori_idx and fixation_mask[current_idx] == 0:\n",
    "                current_idx -= 1\n",
    "            end_fixation = fixation_mask[current_idx]\n",
    "        # the mask are saved shifted in order to assign 0 to the saccade samples\n",
    "        start_fixation -= 1\n",
    "        end_fixation -= 1\n",
    "        x = noisy_samples[:, down_idx:down_idx + size]\n",
    "        y = fixations[:, start_fixation: end_fixation + 1]\n",
    "        return x, y, start_fixation, end_fixation\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Fetches a single sample from RAM, applies sampling and noise.\n",
    "        \"\"\"\n",
    "        # Get the pre-loaded data for this index\n",
    "        down_gaze = self.data_store['down_gaze'][index].reshape((3, -1))\n",
    "        fixations = self.data_store['fixations'][index].reshape((3, -1))\n",
    "\n",
    "        x = down_gaze\n",
    "        y = fixations\n",
    "\n",
    "        if self.sample_size != -1:\n",
    "            fixation_mask = self.data_store['fixation_mask'][index]\n",
    "            x, y, start_fixation, end_fixation = self.__extract_random_period(\n",
    "                self.sample_size,\n",
    "                x,\n",
    "                fixations,\n",
    "                fixation_mask\n",
    "            )\n",
    "            \n",
    "        # Apply noise augmentation\n",
    "        # (Assuming your noise function can process a single [3, N] array)\n",
    "        x, _ = add_random_center_correlated_radial_noise(x, [320//2, 512//2], 1/16,\n",
    "                                                                radial_corr=.2,\n",
    "                                                                radial_avg_norm=4.13,\n",
    "                                                                radial_std=3.5,\n",
    "                                                                center_noise_std=100,\n",
    "                                                                center_corr=.3,\n",
    "                                                                center_delta_norm=300,\n",
    "                                                                center_delta_r=.3)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e60884",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0f8df",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f33bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PathCocoFreeViewDatasetBatch(sample_size= 8,log = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eed931c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.close_and_remove_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12c2e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in memory\n"
     ]
    }
   ],
   "source": [
    "datasetv2 = InMemoryDataset(sample_size= 8,log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf6ba0",
   "metadata": {},
   "source": [
    "## Speed Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83410f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39msample_count())):\n\u001b[0;32m      2\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mget_single_item(i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(dataset.sample_count())):\n",
    "    dataset.get_single_item(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9c9497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39869/39869 [00:00<00:00, 49042.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(datasetv2))):\n",
    "    datasetv2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092f67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def seq2seq_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for a seq-to-seq task with variable sequence lengths.\n",
    "\n",
    "    Pads the input (encoder) sequences and the target (decoder) sequences \n",
    "    to the maximum length found in the current batch.\n",
    "\n",
    "    Args:\n",
    "        batch: A list of tuples, where each tuple is (input_seq, target_seq).\n",
    "               - input_seq: torch.Tensor of shape (L_in,)\n",
    "               - target_seq: torch.Tensor of shape (L_target,)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the batched and padded tensors.\n",
    "    \"\"\"\n",
    "    # print('Inside collate')\n",
    "    # print(type(batch[0][0]))\n",
    "    # print(batch[0][0].shape)\n",
    "    # print(batch[0][1].shape)\n",
    "    # 1. Separate inputs and targets\n",
    "    input_sequences = [torch.from_numpy(item[0].T).float() for item in batch]\n",
    "    target_sequences = [torch.from_numpy(item[1].T).float() for item in batch]\n",
    "\n",
    "    PAD_TOKEN_ID = 0.0\n",
    "\n",
    "    # 4. Pad sequences\n",
    "    # torch.nn.utils.rnn.pad_sequence is the most straightforward way\n",
    "    # batch_first=True makes the output shape (Batch_Size, Max_Length)\n",
    "    \n",
    "    padded_inputs = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_sequences, \n",
    "        batch_first=True, \n",
    "        padding_value=PAD_TOKEN_ID\n",
    "    )\n",
    "    \n",
    "    padded_targets = torch.nn.utils.rnn.pad_sequence(\n",
    "        target_sequences, \n",
    "        batch_first=True, \n",
    "        padding_value=PAD_TOKEN_ID\n",
    "    )\n",
    "    # print('finishing collate')\n",
    "    # 5. Return the collated batch\n",
    "    return padded_inputs, padded_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080842e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:01<00:00, 244.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataloader = DataLoader(datasetv2, batch_size=128, shuffle=True, num_workers=0, collate_fn= seq2seq_collate_fn)\n",
    "for batch in tqdm(dataloader):\n",
    "    x,y = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea8ec4",
   "metadata": {},
   "source": [
    "## Batch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85e037db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading original data\n",
      "shuffled data saved\n"
     ]
    }
   ],
   "source": [
    "dataset.close_and_remove_data()\n",
    "dataset.shuffle_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34412f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:01<00:00, 308.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO The multiprocessing dataloader (sharing the hdf5 files through multitable)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    dataset[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
